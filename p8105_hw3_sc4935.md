p8105\_hw3\_sc4935
================
Shiwei Chen
10/13/2021

# Problem 1

Load dataset of instacart.

``` r
library(p8105.datasets)
data("instacart")
```

How many aisles are there, and which aisles are the most items ordered
from?

``` r
instacart_df = count(instacart, aisle) %>% 
  mutate(aisle_ranking = min_rank(desc(n)))
instacart_df 
```

    ## # A tibble: 134 × 3
    ##    aisle                      n aisle_ranking
    ##    <chr>                  <int>         <int>
    ##  1 air fresheners candles  1067           109
    ##  2 asian foods             7007            53
    ##  3 baby accessories         306           132
    ##  4 baby bath body care      328           131
    ##  5 baby food formula      13198            26
    ##  6 bakery desserts         1501            99
    ##  7 baking ingredients     13088            27
    ##  8 baking supplies decor   1094           106
    ##  9 beauty                   287           134
    ## 10 beers coolers           1839            90
    ## # … with 124 more rows

``` r
nrow(instacart_df)
```

    ## [1] 134

``` r
arrange(instacart_df, aisle_ranking) %>% 
  filter(aisle_ranking == 1)
```

    ## # A tibble: 1 × 3
    ##   aisle                 n aisle_ranking
    ##   <chr>             <int>         <int>
    ## 1 fresh vegetables 150609             1

``` r
view(instacart_df)
```

So, there are 134 kinds of aisles, and by arranging them, the most items
ordered from fresh vegetables.

Make a plot that shows the number of items ordered in each aisle,
limiting this to aisles with more than 10000 items ordered. Arrange
aisles sensibly, and organize your plot so others can read it.

``` r
instacart_df %>% 
  filter(n > 10000) %>% 
  ggplot(aes(x = n, y = aisle)) +
  geom_point() + 
  labs(
    title = "The plot of items in each aisle",
    x = "Number of items ordered in each aisle",
    y = "Aisle name",
    caption = "Data from instacart of the p8105.datasets package"
  )
```

![](p8105_hw3_sc4935_files/figure-gfm/unnamed-chunk-4-1.png)<!-- -->

Make a table showing the three most popular items in each of the aisles
“baking ingredients”, “dog food care”, and “packaged vegetables fruits”.
Include the number of times each item is ordered in your table.

``` r
instacart_table1_df = instacart %>% 
  select(aisle, product_name) %>% 
  filter(aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits") %>% 
  group_by(aisle, product_name) %>% 
  summarize(order_times = n()) %>% 
  mutate(product_rank = min_rank(desc(order_times))) %>% 
  filter(product_rank == 1) %>% 
  select(-product_rank) %>% 
  pivot_wider(
  names_from = "product_name", 
  values_from = "order_times") %>% 
  view()
```

    ## `summarise()` has grouped output by 'aisle'. You can override using the `.groups` argument.

``` r
knitr::kable(instacart_table1_df)
```

| aisle                      | Light Brown Sugar | Snack Sticks Chicken & Rice Recipe Dog Treats | Organic Baby Spinach |
|:---------------------------|------------------:|----------------------------------------------:|---------------------:|
| baking ingredients         |               499 |                                            NA |                   NA |
| dog food care              |                NA |                                            30 |                   NA |
| packaged vegetables fruits |                NA |                                            NA |                 9784 |

``` r
instacart_table1_df 
```

    ## # A tibble: 3 × 4
    ## # Groups:   aisle [3]
    ##   aisle                      `Light Brown Sug… `Snack Sticks C… `Organic Baby S…
    ##   <chr>                                  <int>            <int>            <int>
    ## 1 baking ingredients                       499               NA               NA
    ## 2 dog food care                             NA               30               NA
    ## 3 packaged vegetables fruits                NA               NA             9784

Make a table showing the mean hour of the day at which Pink Lady Apples
and Coffee Ice Cream are ordered on each day of the week; format this
table for human readers (i.e. produce a 2 x 7 table).

``` r
instacart_table2_df = instacart %>% 
  select(order_dow, order_hour_of_day, product_name) %>% 
  filter(product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  group_by(order_dow, product_name) %>% 
  summarise(mean_hours = mean(order_hour_of_day)) %>% 
  mutate(order_dow = recode(order_dow, "0" = "Sunday", "1" = "Monday", "2" = "Tuesday", "3" = "Wednesday", "4" = "Thursday", "5" = "Friday", "6" = "Saturday")) %>% 
  pivot_wider(
  names_from = "order_dow", 
  values_from = "mean_hours") %>% 
  view()
```

    ## `summarise()` has grouped output by 'order_dow'. You can override using the `.groups` argument.

``` r
knitr::kable(instacart_table2_df)
```

| product\_name    |   Sunday |   Monday |  Tuesday | Wednesday | Thursday |   Friday | Saturday |
|:-----------------|---------:|---------:|---------:|----------:|---------:|---------:|---------:|
| Coffee Ice Cream | 13.77419 | 14.31579 | 15.38095 |  15.31818 | 15.21739 | 12.26316 | 13.83333 |
| Pink Lady Apples | 13.44118 | 11.36000 | 11.70213 |  14.25000 | 11.55172 | 12.78431 | 11.93750 |

``` r
instacart_table2_df
```

    ## # A tibble: 2 × 8
    ##   product_name     Sunday Monday Tuesday Wednesday Thursday Friday Saturday
    ##   <chr>             <dbl>  <dbl>   <dbl>     <dbl>    <dbl>  <dbl>    <dbl>
    ## 1 Coffee Ice Cream   13.8   14.3    15.4      15.3     15.2   12.3     13.8
    ## 2 Pink Lady Apples   13.4   11.4    11.7      14.2     11.6   12.8     11.9

Write a short description of the dataset, noting the size and structure
of the data, describing some key variables, and giving illstrative
examples of observations.

# Problem 2

Load dataset of BRFSS (brfss\_smart2010).

``` r
library(p8105.datasets)
data("brfss_smart2010")
```

First, do some data cleaning.

format the data to use appropriate variable names. focus on the “Overall
Health” topic. include only responses from “Excellent” to “Poor”.
organize responses as a factor taking levels ordered from “Poor” to
“Excellent”.

``` r
brfss_cleaned_df = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  select(-locationabbr) %>% 
  separate(locationdesc, into = c("state", "place"), sep = " - ") %>% 
  filter(topic == "Overall Health") %>% 
  filter(response == "Excellent" | response == "Very good" | response == "Good" | response == "Fair" | response == "Poor") %>%
  mutate(response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent"))) %>% 
  view()

brfss_cleaned_df  
```

    ## # A tibble: 10,625 × 23
    ##     year state place            class topic question response sample_size data_value
    ##    <int> <chr> <chr>            <chr> <chr> <chr>    <fct>          <int>      <dbl>
    ##  1  2010 AL    Jefferson County Heal… Over… How is … Excelle…          94       18.9
    ##  2  2010 AL    Jefferson County Heal… Over… How is … Very go…         148       30  
    ##  3  2010 AL    Jefferson County Heal… Over… How is … Good             208       33.1
    ##  4  2010 AL    Jefferson County Heal… Over… How is … Fair             107       12.5
    ##  5  2010 AL    Jefferson County Heal… Over… How is … Poor              45        5.5
    ##  6  2010 AL    Mobile County    Heal… Over… How is … Excelle…          91       15.6
    ##  7  2010 AL    Mobile County    Heal… Over… How is … Very go…         177       31.3
    ##  8  2010 AL    Mobile County    Heal… Over… How is … Good             224       31.2
    ##  9  2010 AL    Mobile County    Heal… Over… How is … Fair             120       15.5
    ## 10  2010 AL    Mobile County    Heal… Over… How is … Poor              66        6.4
    ## # … with 10,615 more rows, and 14 more variables: confidence_limit_low <dbl>,
    ## #   confidence_limit_high <dbl>, display_order <int>, data_value_unit <chr>,
    ## #   data_value_type <chr>, data_value_footnote_symbol <chr>,
    ## #   data_value_footnote <chr>, data_source <chr>, class_id <chr>,
    ## #   topic_id <chr>, location_id <chr>, question_id <chr>, respid <chr>,
    ## #   geo_location <chr>

In 2002, which states were observed at 7 or more locations?

``` r
brfss_2002 = filter(brfss_cleaned_df, year == 2002) %>% 
  group_by(state) %>% 
  summarize(observed = n_distinct(place)) %>% 
  filter(observed >= 7) %>% 
  view()
brfss_2002
```

    ## # A tibble: 6 × 2
    ##   state observed
    ##   <chr>    <int>
    ## 1 CT           7
    ## 2 FL           7
    ## 3 MA           8
    ## 4 NC           7
    ## 5 NJ           8
    ## 6 PA          10

What about in 2010?

``` r
brfss_2010 = filter(brfss_cleaned_df, year == 2010) %>% 
  group_by(state) %>% 
  summarize(observed = n_distinct(place)) %>% 
  filter(observed >= 7) %>% 
  view()
brfss_2010
```

    ## # A tibble: 14 × 2
    ##    state observed
    ##    <chr>    <int>
    ##  1 CA          12
    ##  2 CO           7
    ##  3 FL          41
    ##  4 MA           9
    ##  5 MD          12
    ##  6 NC          12
    ##  7 NE          10
    ##  8 NJ          19
    ##  9 NY           9
    ## 10 OH           8
    ## 11 PA           7
    ## 12 SC           7
    ## 13 TX          16
    ## 14 WA          10

Construct a dataset that is limited to Excellent responses, and
contains, year, state, and a variable that averages the data\_value
across locations within a state. Make a “spaghetti” plot of this average
value over time within a state (that is, make a plot showing a line for
each state across years – the geom\_line geometry and group aesthetic
will help).

``` r
excellent_df = filter(brfss_cleaned_df, response == "Excellent") %>% 
  select(year, state, place, data_value) %>% 
  group_by(year, state) %>% 
  summarise(mean_dv = mean(data_value)) %>% 
  view()
```

    ## `summarise()` has grouped output by 'year'. You can override using the `.groups` argument.

``` r
excellent_df
```

    ## # A tibble: 443 × 3
    ## # Groups:   year [9]
    ##     year state mean_dv
    ##    <int> <chr>   <dbl>
    ##  1  2002 AK       27.9
    ##  2  2002 AL       18.5
    ##  3  2002 AR       24.1
    ##  4  2002 AZ       24.1
    ##  5  2002 CA       22.7
    ##  6  2002 CO       23.1
    ##  7  2002 CT       29.1
    ##  8  2002 DC       29.3
    ##  9  2002 DE       20.9
    ## 10  2002 FL       25.7
    ## # … with 433 more rows

``` r
ggplot(excellent_df, aes(x = year, y = mean_dv, color = state)) +
  geom_point(alpha = 0.5) + geom_line(alpha = 0.5) + 
  labs(
    title = "The average value across locations over time within a state",
    x = "Average of the data_value",
    y = "Year",
    caption = "Data from brfss_smart2010 of the p8105.datasets package"
  ) 
```

    ## Warning: Removed 4 rows containing missing values (geom_point).

    ## Warning: Removed 3 row(s) containing missing values (geom_path).

![](p8105_hw3_sc4935_files/figure-gfm/unnamed-chunk-11-1.png)<!-- -->

Make a two-panel plot showing, for the years 2006, and 2010,
distribution of data\_value for responses (“Poor” to “Excellent”) among
locations in NY State.

``` r
two_panel_plot = brfss_cleaned_df %>% 
  filter(state == "NY") %>% 
  filter(year == 2006 | year == 2010) %>% 
  select(year, state, place, response, data_value) %>% 
  view()

two_panel_plot
```

    ## # A tibble: 75 × 5
    ##     year state place        response  data_value
    ##    <int> <chr> <chr>        <fct>          <dbl>
    ##  1  2010 NY    Bronx County Excellent       17.6
    ##  2  2010 NY    Bronx County Very good       25.9
    ##  3  2010 NY    Bronx County Good            35.1
    ##  4  2010 NY    Bronx County Fair            16.7
    ##  5  2010 NY    Bronx County Poor             4.7
    ##  6  2010 NY    Erie County  Excellent       17.2
    ##  7  2010 NY    Erie County  Very good       37.9
    ##  8  2010 NY    Erie County  Good            29.6
    ##  9  2010 NY    Erie County  Fair            12.7
    ## 10  2010 NY    Erie County  Poor             2.5
    ## # … with 65 more rows

``` r
ggplot(two_panel_plot, aes(x = data_value, y = response, color = place)) + 
  geom_point(alpha = 1) +
  geom_smooth(se = FALSE) + 
  facet_grid(. ~ year) +
  labs(
    title = "Distribution of data_value for responses among locations in NY for 2006 and 2010",
    x = "Data_value",
    y = "Responses",
    caption = "Data from brfss_smart2010 of the p8105.datasets package"
  ) 
```

    ## `geom_smooth()` using method = 'loess' and formula 'y ~ x'

![](p8105_hw3_sc4935_files/figure-gfm/unnamed-chunk-12-1.png)<!-- -->

# Problem 3

Load dataset of accel\_data.

Load, tidy, and otherwise wrangle the data. Your final dataset should
include all originally observed variables and values; have useful
variable names; include a weekday vs weekend variable; and encode data
with reasonable variable classes. Describe the resulting dataset
(e.g. what variables exist, how many observations, etc).

``` r
accel_df = read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  select(-day_id) %>% 
  mutate(day = factor(day, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday",  "Sunday"))) %>% 
  arrange(week, day) %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "counts", 
    values_to = "activity") %>% 
  mutate(weekday_vs_weekend = if_else(day == "Saturday" | day == "Sunday", "weekend", "weekday")) %>%
  separate(counts, into = c("remove", "minutes"), sep = "_") %>% 
  select(-remove) %>% 
  mutate(week = as.character(week)) %>% 
  mutate(minutes = as.numeric(minutes)) %>% 
  view()
```

    ## Rows: 35 Columns: 1443

    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr    (1): day
    ## dbl (1442): week, day_id, activity.1, activity.2, activity.3, activity.4, ac...

    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
accel_df  
```

    ## # A tibble: 50,400 × 5
    ##    week  day    minutes activity weekday_vs_weekend
    ##    <chr> <fct>    <dbl>    <dbl> <chr>             
    ##  1 1     Monday       1        1 weekday           
    ##  2 1     Monday       2        1 weekday           
    ##  3 1     Monday       3        1 weekday           
    ##  4 1     Monday       4        1 weekday           
    ##  5 1     Monday       5        1 weekday           
    ##  6 1     Monday       6        1 weekday           
    ##  7 1     Monday       7        1 weekday           
    ##  8 1     Monday       8        1 weekday           
    ##  9 1     Monday       9        1 weekday           
    ## 10 1     Monday      10        1 weekday           
    ## # … with 50,390 more rows

Traditional analyses of accelerometer data focus on the total activity
over the day. Using your tidied dataset, aggregate accross minutes to
create a total activity variable for each day, and create a table
showing these totals. Are any trends apparent?

``` r
accel_df %>% 
  group_by(week, day) %>% 
  summarise(total_activity = sum(activity)) %>% 
   pivot_wider(
  names_from = "day", 
  values_from = "total_activity") %>% 
  view() %>% 
  knitr::kable()
```

    ## `summarise()` has grouped output by 'week'. You can override using the `.groups` argument.

| week |    Monday |  Tuesday | Wednesday | Thursday |   Friday | Saturday | Sunday |
|:-----|----------:|---------:|----------:|---------:|---------:|---------:|-------:|
| 1    |  78828.07 | 307094.2 |    340115 | 355923.6 | 480542.6 |   376254 | 631105 |
| 2    | 295431.00 | 423245.0 |    440962 | 474048.0 | 568839.0 |   607175 | 422018 |
| 3    | 685910.00 | 381507.0 |    468869 | 371230.0 | 467420.0 |   382928 | 467052 |
| 4    | 409450.00 | 319568.0 |    434460 | 340291.0 | 154049.0 |     1440 | 260617 |
| 5    | 389080.00 | 367824.0 |    445366 | 549658.0 | 620860.0 |     1440 | 138421 |

Accelerometer data allows the inspection activity over the course of the
day. Make a single-panel plot that shows the 24-hour activity time
courses for each day and use color to indicate day of the week. Describe
in words any patterns or conclusions you can make based on this graph.

``` r
accel_df %>% 
  ggplot(aes(x = minutes, y = activity, color = day)) + 
  geom_point(alpha = 0.5) +
  labs(
    title = "24-hour activity time courses for each day",
    x = "Minutes of a day",
    y = "Activity count",
    caption = "Data from accel_data.csv"
  ) 
```

![](p8105_hw3_sc4935_files/figure-gfm/unnamed-chunk-15-1.png)<!-- -->
